{"cells":[{"metadata":{"_cell_guid":"09012269-7819-4b10-bd09-6c989f2a1ba2","_uuid":"a5cd79dbdec221d5a14f0748afb0322f02d592b9"},"cell_type":"markdown","source":"* [Day 1: Reading in common data file formats: .json, .txt and .xlsx](https://www.kaggle.com/rtatman/data-cleaning-challenge-json-txt-and-xls/)\n* [Day 2: Filling in missing values ](https://www.kaggle.com/rtatman/data-cleaning-challenge-imputing-missing-values/)\n* [Day 3: Identifying & handling outliers](https://www.kaggle.com/rtatman/data-cleaning-challenge-outliers/)\n* [Day 4: Removing duplicate records](https://www.kaggle.com/rtatman/data-cleaning-challenge-deduplication/)\n* [Day 5: Cleaning numbers (percentages, money, dates and times)](https://www.kaggle.com/rtatman/data-cleaning-challenge-cleaning-numeric-columns/)\n    \n___\n\nWelcome to Day 4 of the 5-Day Data Challenge! Today, we're going to be looking at **deduplication**.\n\n\"Duplication\" just means that you have repeated data in your dataset. This could be due to things like data entry errors or data collection methods. For example, if you're using a web scraper you may happen to scrape the same webpage more than once, or the same information from two different pages. \nWhatever the reason, deduplication can lead you to make incorrect conclusions by leading you to believe that some observations are more common than they really are.\n\nToday we're going to learn how to find and remove duplicate records. (Removing duplicates is called \"deduplication\".) Here's a quick overview of what we'll be doing today.\n\n* Visualizing duplication \n* Finding & removing exact duplicates\n* Finding & removing partial duplicates\n\nI'll start by introducing each concept or technique, and then you'll get a chance to apply it with an exercise (look for the **Your turn!** section). Ready? Let's get started!\n\n___\n\n**Kernel FAQs:**\n\n* **How do I get started?**   To get started, click the blue \"Fork Notebook\" button in the upper, right hand corner. This will create a private copy of this notebook that you can edit and play with. Once you're finished with the exercises, you can choose to make your notebook public to share with others. :)\n\n* **How do I run the code in this notebook?** Once you fork the notebook, it will open in the notebook editor. From there you can write code in any code cell (the ones with the grey background) and run the code by either 1) clicking in the code cell and then hitting CTRL + ENTER or 2) clicking in the code cell and the clicking on the white \"play\" arrow to the left of the cell. If you want to run all the code in your notebook, you can use the double, \"fast forward\" arrows at the bottom of the notebook editor.\n\n* **How do I save my work?** Any changes you make are saved automatically as you work. You can run all the code in your notebook and save a static version by hitting the blue \"Commit & Run\" button in the upper right hand corner of the editor. \n\n* **How can I find my notebook again later?** The easiest way is to go to your user profile (https://www.kaggle.com/replace-this-with-your-username), then click on the \"Kernels\" tab. All of your kernels will be under the \"Your Work\" tab, and all the kernels you've upvoted will be under the \"Favorites\" tab.\n\n___\n"},{"metadata":{"_cell_guid":"640f8797-70e7-4fad-8723-66e58476e658","_uuid":"5df79f2b00a3dbd89d1c9a21fbb7564a93a6ba52"},"cell_type":"markdown","source":"# Get our environment set up\n___\n\nBefore we get started, we need to get our environment set up. Today, we'll be working with two video game datasets. The `video_game` dataset contains information on what games some Steam users bought and how many hours they spent playing them. The `ign_data` dataset contains information on reviews of different video games."},{"metadata":{"_cell_guid":"06e4218b-40f8-49a3-bc72-124294f6b4b7","_uuid":"7e8509598680a6471093d84c88cb81c5c44f243c","_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"# libraries we'll need\nlibrary(tidyverse) # handy utility functions\nlibrary(scrubr) # for deduplication\n\n# read in Steam video game data & add column names\nvideo_games <- read.csv(\"../input/steam-video-games/steam-200k.csv\", header = F) # read in data\nnames(video_games) <- c(\"user_id\", \"game_title\", \"behavior_name\", \"value\", \"x\") # add column names\nvideo_games <- as_data_frame(video_games[, 1:4]) # drop last column & convert to data_frame\n\n# read in IGN video game data\nign_data <- read_csv(\"../input/20-years-of-games/ign.csv\")\nign_data <- select(ign_data, -X1) # drop column with row numbers","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"080a7924-925c-44e0-afca-60ec47db59ba","_uuid":"3a91700bfc5ef8bc655a0f83fe08785027fd172b"},"cell_type":"markdown","source":"# What is duplication?\n___\n\nTo start off, let's define what we mean by \"duplication\". Duplication can mean two slightly different things: \n\n1.  More than one record that is exactly the same. This is what I call \"exact duplication\".\n2. More than one record associated with the same observation, but the values in the rows are not exactly the same. This is what I call \"partial duplication\", but removing these types of duplicated records is also called \"record linkage\". \n\nToday, we'll be talking about methods to identify and remove both of these types of duplication. "},{"metadata":{"_cell_guid":"54dece50-b47b-4eb1-adc7-30d929ff68ca","_uuid":"8472c478f51d6f4cbd77c6b06e66bf86b4eaef2b"},"cell_type":"markdown","source":"# Visualizing duplication \n____\n\nYou may have noticed that I tend to do a lot of visualizing as part of different data cleaning tasks, and deduplication is no exception! Let's take a look at how many rows are duplicated in our dataset and where they are. There are a couple reasons you may want to do this:\n\n1. See how much duplicated data you have. If you only have a couple duplicates, or even none, you can just move on without worrying about them.\n2. See if there are any patterns in duplication. One fairly common pattern is that you'll see big blocks of duplicates due to data being copy and pasted at the end of existing data. If that's the case, you can just remove those rows from your dataframe and call it a day.\n\nTo plot duplicates, I'm first going to create a dataframe with 1) a logical vector indicating whether or not a specific row is duplicated elsewhere in the dataset and 2) a numeric vector of the index of each row. (I'm not using row numbers because if you're using the Tidyverse version of a dataframe, the get removed whenever you manipulate the dataframe.) Then, I'm going to plot that information so that each duplicated row shows up as a black line. Like so:  "},{"metadata":{"_cell_guid":"38e50e7a-4be8-45ea-b7cc-43829b18196a","_uuid":"6096d7c96d65807d4080160151cbce693d0ed4c5","trusted":false},"cell_type":"code","source":"# get the row numbers of duplicated rows\nduplicated_rows <- data_frame(duplicated = duplicated(ign_data), row = 1:nrow(ign_data)) %>%\n    filter(duplicated == T)\n\n# Plot duplicated rows as black lines\nggplot(duplicated_rows, aes(xintercept = row)) +\n    geom_vline(aes(xintercept = row)) + # plot a black line for each duplicated row\n    ggtitle(\"Indexes of duplicated rows\") + # add a title\n    coord_flip() + scale_x_reverse() #flip x & y axis and reverse the x axis","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b81d84b1-eab2-422b-b7a6-79784128cddb","_uuid":"48ef4473aec12d3fd88eab975ef9084be264bd50"},"cell_type":"markdown","source":"Looking at this chart tells us two thing. First, we do have quick a bit of duplicated data, so we'll probably want to handle that somehow. Secondly, there's no clear pattern to the duplicated values, so it would be too time consuming to remove them by row index."},{"metadata":{"_cell_guid":"3a6625f8-4a3a-489d-b522-338db2769656","_uuid":"338401ddf025375ed1829c18c6c923814d351820"},"cell_type":"markdown","source":"## Your turn!\n___\n\nPlot the indexes of duplicated rows in the `video games` dataset. Are there more or less duplicated rows in this dataset than in the IGN dataset? Do you notice any patterns?"},{"metadata":{"_cell_guid":"f272b162-2fdf-4f0b-97c7-21ab4aab6514","_uuid":"2044cf4dba354f1fa3b9dcf0173fbdcaf5ecd290","trusted":false},"cell_type":"code","source":"# your code here :)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7eb5a358-f644-41e3-aac3-ccd83f672f43","_uuid":"3641219d0283338584faeb22b4aeed031459a6b1"},"cell_type":"markdown","source":"# Find and remove exact duplicates\n____\n\nNow that we know how to find them, let's look at some of our data."},{"metadata":{"_cell_guid":"3f865a56-8d74-4d53-b4a4-636f24e9ca04","_uuid":"5b1f7b9961d8e3af7f2bb54da05ed7cbd94467da","trusted":false},"cell_type":"code","source":"# look at the first few rows\nhead(ign_data)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5487e5fb-399b-485c-b1ba-48dd3415f7bf","_uuid":"c20d9bcd7fa22da0d57e38a4fd5317ac94cfc7aa"},"cell_type":"markdown","source":"While there are some things that I would call \"partial duplicates\", like the two reviews of \"NHL 13\" with the same score and release date, I don't see any exact duplicates. We know they exist because we saw them in our visualization earlier, though. Let's use the `duplicated()` function to see how many exactly duplicated rows we have in our dataset:"},{"metadata":{"_cell_guid":"5073db87-9383-41f4-b168-a0269debb620","_uuid":"b150d87e393a076bc423c432c43ed1167850ccf8","trusted":false},"cell_type":"code","source":"# count the number of duplicated row in our dataset\nduplicated(ign_data) %>%\n    sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4d3fbc58-4963-43d7-8afa-241dee7eab15","_uuid":"6e6a9ab9186a31d3f8136885d08058124fb2dd92"},"cell_type":"markdown","source":"So it looks like there are 48 duplicated rows in our dataset. Fortunately, they are very easy to remove: you can the function `distinct()` to select only the non-duplicated rows in our dataset. Like so:"},{"metadata":{"_cell_guid":"ca34935b-4332-427c-a852-144b61049332","_uuid":"48023a34f5c631025e43a4ac9df432bac7baf1f6","trusted":false},"cell_type":"code","source":"# get only the distinct rows in our dataset\nign_data_distinct <- distinct(ign_data)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d1d01be3-1126-4308-87ed-eba478579e16","_uuid":"009bd32affc481a39e563eca8ba48c8070143866"},"cell_type":"markdown","source":"Just to double-check that we did remove all the duplicated rows, let's see how many rows we removed from our dataframe using `distinct()`. "},{"metadata":{"_cell_guid":"bcd76df6-a32d-4c51-93a9-b0690f7cd7b5","_uuid":"e5ae1cdfb9d822cd0880f070bb817da0762b6e60","trusted":false},"cell_type":"code","source":"# how many rows did we remove?\nnrow(ign_data) - nrow(ign_data_distinct)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0aba99a5-1525-42dd-bc8a-66065f4e017b","_uuid":"97d2abb81fd2d91f5afb11646e4c5999ccd00d58"},"cell_type":"markdown","source":"Success! We did remove all 48 rows with exactly duplicates from our dataframe."},{"metadata":{"_cell_guid":"10abff79-989f-46e2-9be0-5080d910df99","_uuid":"e87b72284bcfd53ddf609c72bdec53b0226e6b7c"},"cell_type":"markdown","source":"## Your turn!\n___\n\nAre there any exact duplicates in the `video_games` dataset? If so, try removing them."},{"metadata":{"_cell_guid":"e8bc18e6-fb18-46b3-a8e9-59adc90ec996","_uuid":"4aa37411744c7ec77b433139984724dfe7eeccd5","trusted":false},"cell_type":"code","source":"# your code here","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d1dbf4de-8c36-49a1-9abf-fb672a8d96bb","_uuid":"d8b1f09feae8bbcbb7b43660cb42a86897ce98d0"},"cell_type":"markdown","source":"# Find and remove partial duplicates\n___\n\nRemoving *exact* duplicates is pretty straightforward. Removing partial duplicates, on the other hand, is much more complex. Fortunately, there are some handy tools we can use to make the process easier. I particularly like the [the dedup() function from the scrubr package](https://cran.r-project.org/web/packages/scrubr/vignettes/scrubr_vignette.html) by Scott Chamberlain. (If you haven't run into it before, the scrubr package can help you with a lot of different data cleaning tasks, like removing impossible latitude and longitude coordinates or standardizing dates.)\n\nOne of the hassles of removing partial duplicates is that it tends to take a long time. This is because you need to make a long of different comparisons to determine which rows only partially match with other ones. It's also because, in this case, we're actually converting every row to a string and then [comparing how similar the strings are](https://www.rdocumentation.org/packages/qlcMatrix/versions/0.9.5/topics/sim.strings) (this can help you catch things like slightly misspelled names). Since it takes longer the larger your dataset it, I'm going to use just the first twenty rows of the IGN dataset for this example so it doesn't take forever. "},{"metadata":{"_cell_guid":"56eea978-ec22-4a6c-a937-91bb9c3cc319","_uuid":"5b14226bdcf1e41a973aedd1ee1cfe6737d33849","trusted":false},"cell_type":"code","source":"# look at just the first 20 rows\nign_data[1:20,]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a675c9b1-e197-4188-8c2c-4cbc48471afe","_uuid":"94943e721a3e13d782d38eddf8518cf76d9c234d"},"cell_type":"markdown","source":"As you can see, we have a lot of rows that are *almost* the same. Let's see how many of them `dedup()` can catch."},{"metadata":{"_cell_guid":"ca76a03a-1e58-4e0b-8964-baa15d003319","_uuid":"0942107294b902692424e65fa34182901a122d8a","trusted":false},"cell_type":"code","source":"# automatically remove duplicated values, including partials\nign_data_deduped <- ign_data[1:20,] %>%\n    dedup()\n\n# see duplicate records\nattr(ign_data_deduped, \"dups\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef6943c9-5405-4edc-8066-86abba9ac320","_uuid":"aaaf74987a6a7b8000f00f6349eb72348c13e609"},"cell_type":"markdown","source":"So we are seeing some partial matches, but not all of them. This is partly because the default tolerance argument for for `dedup()` is pretty high, so it's not picking out all the possible partial matches. Lower tolerance will identify more records are duplicates and higher tolerance will identify fewer.\n\n> [**Tolerance**: Score (0 to 1) at which to determine a match. Youâ€™ll want to inspect outputs closely to tweak this value based on your data, as results can vary.](https://cran.r-project.org/web/packages/scrubr/scrubr.pdf)\n\nLet's try again with a lower tolerance and see if we can catch more partial matches:"},{"metadata":{"_cell_guid":"f779520f-aceb-4911-89b2-429ed0af5f3c","_uuid":"197de5a26080c4f70f05e0a5b4d080caeb211335","trusted":false},"cell_type":"code","source":"# automatically remove duplicated values, including partials\nign_data_deduped <- ign_data[1:20,] %>%\n    dedup(tolerance = 0.5)\n\n# see duplicate records\nattr(ign_data_deduped, \"dups\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"57597eeb-9445-4aee-be0d-d5f116f09934","_uuid":"a63ff73e3a7c04fd35fe171f3c8a5da230ad60c2"},"cell_type":"markdown","source":"That looks much better to me! From here on, I can just use the `ign_data_deduped` dataframe when working with my data. "},{"metadata":{"_cell_guid":"7587d4c8-240d-4446-b848-26d6efb7be1a","_uuid":"a901baf04550de2fa9492c17c42f1566ab31003a"},"cell_type":"markdown","source":"## Your turn!\n___\n\nAre there any exact duplicates in the `video_games` dataset? If so, try removing them.\n\nThen try removing partial duplicates using `dedup()`.  Try adjusting the tolerance. If you're feeling very ambitious, try recording & graphing how different tolerance values map to the number of duplicates detected and removed."},{"metadata":{"_cell_guid":"9feaffc7-b992-4ea2-968a-ff3c39561e15","_uuid":"02bbcbb5ff2e55dbe5f47bff3781c8e743b3705f","trusted":false},"cell_type":"code","source":"# your code goes here :)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f317c3c2-67ac-4789-a2da-3543edf01757","_uuid":"67c540ce67fdc9d29b28fdaf703f399d3dc20737"},"cell_type":"markdown","source":"# And that's it for Day 4!\n___\n\nAnd that's it for today! If you have any questions, be sure to post them in the comments below or on the forums.\n\nRemember that your notebook is private by default, and in order to share it with other people or ask for help with it, you'll need to make it public. First, you'll need to save a version of your notebook that shows your current work by hitting the \"Commit & Run\" button. (Your work is saved automatically, but versioning your work lets you go back and look at what it was like at the point you saved it. It also lets you share a nice compiled notebook instead of just the raw code.) Then, once your notebook is finished running, you can go to the Settings tab in the panel to the left (you may have to expand it by hitting the [<] button next to the \"Commit & Run\" button) and setting the \"Visibility\" dropdown to \"Public\". \n\n"}],"metadata":{"language_info":{"version":"3.4.2","file_extension":".r","pygments_lexer":"r","mimetype":"text/x-r-source","codemirror_mode":"r","name":"R"},"kernelspec":{"language":"R","display_name":"R","name":"ir"}},"nbformat":4,"nbformat_minor":1}